{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este ejercicio es que el estudiante aplique modelos ensemble y SVC a un dataset real. Además, que el estudiante pueda analizar las características del dataset y las salidas de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecute el siguiente código y analice la estructura del dataset. Fíjese en la cantidad de datos de entrenamiento existentes, número de atributos, valores máximos, mínimos, correlaciones entre variables, dispersión, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOW_SCALE = \"minmax\"  # standard|minmax  como re-escalar los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"vehicle.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='Class')\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.Class = df.Class.astype('category').cat.codes\n",
    "\n",
    "# genera la matriz de correlaciones entre atributos\n",
    "cor_mat= df[:].corr()\n",
    "mask = np.array(cor_mat)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(30,12)\n",
    "sns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creando las variables `X` y `y`\n",
    "X = df.drop(\"Class\", axis=1).to_numpy()\n",
    "y = df[\"Class\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if HOW_SCALE == \"standard\":\n",
    "    scaler=StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "elif HOW_SCALE == \"minmax\":\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_dataset(x, y):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "    \n",
    "    reduccion = TSNE(n_components=2, init='pca')\n",
    "    x_new = reduccion.fit_transform(X)\n",
    "    tmp_df = pd.DataFrame(np.column_stack([x_new, y]))\n",
    "    tmp_df.columns = [\"x1\", \"x2\", \"Y\"]\n",
    "    \n",
    "    sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"Y\", data=tmp_df, ax=ax)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hay alguna característica importante en el dataset que se deba mencionar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingrese manualmente hyper-parámetros para los siguientes modelos. Además, en los modelos RF y XGBoost, fíjese en el gráfico donde se muestran la importancia de las variables para esos modelos.\n",
    "\n",
    "Genere un gráfico comparando los valores de `accuracy` para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(x_train, y_train)\n",
    "pred = clf_dt.predict(x_test)\n",
    "print(\"Decision Tree: {:.3f}\".format(accuracy_score(pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=5)\n",
    "clf_rf.fit(x_train, y_train)\n",
    "pred = clf_rf.predict(x_test)\n",
    "print(\"Random Forest: {:.3f}\".format(accuracy_score(pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importancia de variables para RandomForest\n",
    "tmp = {\n",
    "    \"variables\": df.drop(\"Class\", axis=1).columns,\n",
    "    \"importancia\": clf_rf.feature_importances_\n",
    "}\n",
    "\n",
    "pd.DataFrame(tmp, index=tmp[\"variables\"]).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=50, learning_rate=0.1)\n",
    "clf.fit(x_train, y_train)\n",
    "pred = clf.predict(x_test)\n",
    "print(\"AdaBoost: {:.3f}\".format(accuracy_score(pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(n_estimators=100, random_state=5, max_depth=5, learning_rate=0.1)\n",
    "clf_gb.fit(x_train, y_train)\n",
    "pred = clf_gb.predict(x_test)\n",
    "print(\"Gradient Boost: {:.3f}\".format(accuracy_score(pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = xgb.XGBClassifier(learning_rate=0.1, max_depth=3, n_estimators=500)\n",
    "clf_xgb.fit(x_train, y_train)\n",
    "pred = clf_xgb.predict(x_test)\n",
    "print(\"XG Boost: {:.3f}\".format(accuracy_score(pred,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importancia de variables para el modelo XBoost\n",
    "tmp = {\n",
    "    \"variables\": df.drop(\"Class\", axis=1).columns,\n",
    "    \"importancia\": clf_xgb.feature_importances_\n",
    "}\n",
    "\n",
    "pd.DataFrame(tmp, index=tmp[\"variables\"]).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = SVC(C=1, gamma=0.1)\n",
    "clf_svc.fit(x_train, y_train)\n",
    "pred = clf_svc.predict(x_test)\n",
    "print(\"SVC: {:.3f}\".format(accuracy_score(pred,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccione los 2 modelos con mejores prestaciones para este problema e intente mejorar su `accuracy`\n",
    "\n",
    "Nota: Realice un tuneado de hyper-parámetros. Además, pruebe si el re-escalamiento estandar ayuda a algún modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando el análisis de correlaciones entre variables en el dataset y la importancia de variables para los modelos de XGBoost y Random Forest, ¿Que se podría hacer para mejorar el `accuracy` para este dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) es una técnica para transformación de datos, cuyo objetivo es maximizar la separación entre 2 clases creando distribuciones gausianas. La documentación de este método se encuentra en https://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html\n",
    "\n",
    "Para este ejercicio, probar LDA para reducir dimensiones del dataset y entrenar 2 modelos: SVC y XGBoost. ¿Que `accuracy` logra encontrar ahora y a cuantas dimensiones se redujo el problema?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
